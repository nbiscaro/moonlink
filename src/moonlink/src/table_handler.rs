/// There're a few LSN concepts used in the table handler:
/// - commit LSN: LSN for a streaming or a non-streaming LSN
/// - flush LSN: LSN for a flush operation
/// - iceberg snapshot LSN: LSN of the latest committed transaction, before which all updates have been persisted into iceberg
/// - table consistent view LSN: LSN if the last handled table event is a commit operation, which indicates mooncake table stays at a consistent view, so table could be flushed safely
/// - replication LSN: LSN come from replication.
///   It's worth noting that there's no guarantee on the numerical order for "replication LSN" and "commit LSN";
///   because if a table recovers from a clean state (aka, all committed messages have confirmed), it's possible to have iceberg snapshot LSN but no further replication LSN.
/// - persisted table LSN: the largest LSN where all updates have been persisted into iceberg
///   Suppose we have two tables, table-A has persisted all updated into iceberg; with table-B taking new updates. persisted table LSN for table-A grows with table-B.
use crate::event_sync::EventSyncSender;
use crate::storage::mooncake_table::delete_vector::BatchDeletionVector;
use crate::storage::mooncake_table::transaction_stream::TransactionStreamCommit;
use crate::storage::mooncake_table::AlterTableRequest;
use crate::storage::mooncake_table::DiskFileEntry;
use crate::storage::mooncake_table::DiskSliceWriter;
use crate::storage::mooncake_table::MaintenanceOption;
use crate::storage::mooncake_table::SnapshotOption;
use crate::storage::mooncake_table::INITIAL_COPY_XACT_ID;
use crate::storage::{io_utils, MooncakeTable};
use crate::table_notify::TableEvent;
use crate::{Error, Result};
use more_asserts as ma;
use tokio::sync::mpsc::{self, Receiver, Sender};
use tokio::sync::watch;
use tokio::task::JoinHandle;
use tokio::time::Duration;
use tracing::Instrument;
use tracing::{debug, error, info_span};
pub mod table_handler_state;
use table_handler_state::{
    MaintenanceProcessStatus, MaintenanceRequestStatus, SpecialTableState, TableHandlerState,
};

/// Handler for table operations
pub struct TableHandler {
    /// Handle to periodical events.
    _periodic_event_handle: JoinHandle<()>,

    /// Handle to the event processing task
    _event_handle: Option<JoinHandle<()>>,

    /// Sender for the table event queue
    event_sender: Sender<TableEvent>,
}

impl TableHandler {
    /// Create a new TableHandler for the given schema and table name
    pub async fn new(
        mut table: MooncakeTable,
        event_sync_sender: EventSyncSender,
        replication_lsn_rx: watch::Receiver<u64>,
        event_replay_tx: Option<mpsc::UnboundedSender<TableEvent>>,
    ) -> Self {
        // Create channel for events
        let (event_sender, event_receiver) = mpsc::channel(100);

        // Create channel for internal control events.
        table.register_table_notify(event_sender.clone()).await;

        // Spawn the task to notify periodical events.
        let event_sender_for_periodical_snapshot = event_sender.clone();
        let event_sender_for_periodical_force_snapshot = event_sender.clone();
        let periodic_event_handle = tokio::spawn(async move {
            let mut periodic_snapshot_interval = tokio::time::interval(Duration::from_millis(500));
            let mut periodic_force_snapshot_interval =
                tokio::time::interval(Duration::from_secs(300));

            loop {
                tokio::select! {
                    // Sending to channel fails only happens when eventloop exits, directly exit timer events.
                    _ = periodic_snapshot_interval.tick() => {
                        if event_sender_for_periodical_snapshot.send(TableEvent::PeriodicalMooncakeTableSnapshot).await.is_err() {
                           return;
                        }
                    }
                    _ = periodic_force_snapshot_interval.tick() => {
                        if event_sender_for_periodical_force_snapshot.send(TableEvent::ForceSnapshot { lsn: None }).await.is_err() {
                            return;
                        }
                    }
                    else => {
                        break;
                    }
                }
            }
        });

        // Spawn the task with the oneshot receiver
        let event_handle = Some(tokio::spawn(
            async move {
                Self::event_loop(
                    event_sync_sender,
                    event_receiver,
                    replication_lsn_rx,
                    event_replay_tx,
                    table,
                )
                .await;
            }
            .instrument(info_span!("table_event_loop")),
        ));

        // Create the handler
        Self {
            _event_handle: event_handle,
            _periodic_event_handle: periodic_event_handle,
            event_sender,
        }
    }

    /// Get the event sender to send events to this handler
    pub fn get_event_sender(&self) -> Sender<TableEvent> {
        self.event_sender.clone()
    }

    /// Main event processing loop
    #[tracing::instrument(name = "table_event_loop", skip_all)]
    async fn event_loop(
        event_sync_sender: EventSyncSender,
        mut event_receiver: Receiver<TableEvent>,
        replication_lsn_rx: watch::Receiver<u64>,
        event_replay_tx: Option<mpsc::UnboundedSender<TableEvent>>,
        mut table: MooncakeTable,
    ) {
        let initial_persistence_lsn = table.get_iceberg_snapshot_lsn();
        let mut table_handler_state = TableHandlerState::new(
            event_sync_sender.table_maintenance_completion_tx.clone(),
            event_sync_sender.force_snapshot_completion_tx.clone(),
            initial_persistence_lsn,
        );

        // Used to clean up mooncake table status, and send completion notification.
        let drop_table = async |table: &mut MooncakeTable, event_sync_sender: EventSyncSender| {
            // Step-1: shutdown the table, which unreferences and deletes all cache files.
            if let Err(e) = table.shutdown().await {
                let _ = event_sync_sender.drop_table_completion_tx.send(Err(e));
                return;
            }

            // Step-2: delete the iceberg table.
            if let Err(e) = table.drop_iceberg_table().await {
                let _ = event_sync_sender.drop_table_completion_tx.send(Err(e));
                return;
            }

            // Step-3: delete the mooncake table.
            if let Err(e) = table.drop_mooncake_table().await {
                let _ = event_sync_sender.drop_table_completion_tx.send(Err(e));
                return;
            }

            // Step-4: send back completion notification.
            let _ = event_sync_sender.drop_table_completion_tx.send(Ok(()));
        };

        // Util function to spawn a detached task to delete evicted data files.
        let start_task_to_delete_evicted = |evicted_file_to_delete: Vec<String>| {
            if evicted_file_to_delete.is_empty() {
                return;
            }
            tokio::task::spawn(async move {
                if let Err(err) = io_utils::delete_local_files(&evicted_file_to_delete).await {
                    error!("Failed to delete object storage cache: {:?}", err);
                }
            });
        };

        // Process events until the receiver is closed or a Shutdown event is received
        loop {
            tokio::select! {
                // Process events from the queue
                Some(event) = event_receiver.recv() => {
                    // Record event if requested.
                    if let Some(replay_tx) = &event_replay_tx {
                        replay_tx.send(event.clone()).unwrap();
                    }

                    table_handler_state.update_table_lsns(&event);

                    match event {
                        event if event.is_ingest_event() => {
                            Self::process_cdc_table_event(event, &mut table, &mut table_handler_state).await;
                        }
                        // ==============================
                        // Interactive blocking events
                        // ==============================
                        //
                        TableEvent::ForceSnapshot { lsn } => {
                            let requested_lsn = if lsn.is_some() {
                                lsn
                            } else if table_handler_state.latest_commit_lsn.is_some() {
                                table_handler_state.latest_commit_lsn
                            } else {
                                None
                            };

                            // Fast-path: nothing to snapshot.
                            if requested_lsn.is_none() {
                                table_handler_state.force_snapshot_completion_tx.send(Some(Ok(/*lsn=*/0))).unwrap();
                                continue;
                            }

                            // Fast-path: if iceberg snapshot requirement is already satisfied, notify directly.
                            let requested_lsn = requested_lsn.unwrap();
                            let last_iceberg_snapshot_lsn = table.get_iceberg_snapshot_lsn();
                            let replication_lsn = *replication_lsn_rx.borrow();
                            let persisted_table_lsn = table_handler_state.get_persisted_table_lsn(last_iceberg_snapshot_lsn, replication_lsn);
                            if persisted_table_lsn >= requested_lsn {
                                table_handler_state.notify_persisted_table_lsn(persisted_table_lsn);
                                continue;
                            }

                            // Iceberg snapshot LSN requirement is not met, record the required LSN, so later commit will pick up.
                            else {
                                table_handler_state.largest_force_snapshot_lsn = Some(match table_handler_state.largest_force_snapshot_lsn {
                                    None => requested_lsn,
                                    Some(old_largest) => std::cmp::max(old_largest, requested_lsn),
                                });
                            }
                        }
                        // Branch to trigger a force regular index merge request.
                        TableEvent::ForceRegularIndexMerge => {
                            // TODO(hjiang): If there's already table maintenance ongoing, skip.
                            if !table_handler_state.can_start_new_maintenance() {
                                let _ = table_handler_state.table_maintenance_completion_tx.send(Ok(()));
                                continue;
                            }
                            // Otherwise queue a request.
                            assert_eq!(table_handler_state.index_merge_request_status, MaintenanceRequestStatus::Unrequested);
                            table_handler_state.index_merge_request_status = MaintenanceRequestStatus::ForceRegular;
                        }
                        // Branch to trigger a force regular data compaction request.
                        TableEvent::ForceRegularDataCompaction => {
                            if !table_handler_state.can_start_new_maintenance() {
                                let _ = table_handler_state.table_maintenance_completion_tx.send(Ok(()));
                                continue;
                            }
                            // Otherwise queue a request.
                            assert_eq!(table_handler_state.data_compaction_request_status, MaintenanceRequestStatus::Unrequested);
                            table_handler_state.data_compaction_request_status = MaintenanceRequestStatus::ForceRegular;
                        }
                        // Branch to trigger a force full index merge request.
                        TableEvent::ForceFullMaintenance => {
                            if !table_handler_state.can_start_new_maintenance() {
                                let _ = table_handler_state.table_maintenance_completion_tx.send(Ok(()));
                                continue;
                            }
                            // Otherwise queue a request.
                            assert_eq!(table_handler_state.index_merge_request_status, MaintenanceRequestStatus::Unrequested);
                            assert_eq!(table_handler_state.data_compaction_request_status, MaintenanceRequestStatus::Unrequested);
                            table_handler_state.data_compaction_request_status = MaintenanceRequestStatus::ForceFull;
                        }
                        // Branch to drop the iceberg table and clear pinned data files from the global object storage cache, only used when the whole table requested to drop.
                        // So we block wait for asynchronous request completion.
                        TableEvent::DropTable => {
                            // Fast-path: no other concurrent events, directly clean up states and ack back.
                            if table_handler_state.can_drop_table_now() {
                                drop_table(&mut table, event_sync_sender).await;
                                return;
                            }

                            // Otherwise, leave a drop marker to clean up states later.
                            table_handler_state.mark_drop_table();
                        }
                        TableEvent::AlterTable { columns_to_drop } => {
                            debug!("altering table, dropping columns: {:?}", columns_to_drop);
                            let alter_table_request = AlterTableRequest {
                                new_columns: vec![],
                                dropped_columns: columns_to_drop,
                            };
                            table_handler_state.start_alter_table(alter_table_request);
                        }
                        TableEvent::StartInitialCopy => {
                            debug!("starting initial copy");
                            table_handler_state.start_initial_copy();
                        }
                        TableEvent::FinishInitialCopy { start_lsn } => {
                            debug!("finishing initial copy");
                            if let Err(e) = table.commit_transaction_stream(INITIAL_COPY_XACT_ID, 0) {
                                error!(error = %e, "failed to finish initial copy");
                            }
                            // Force create the snapshot with LSN 0
                            assert!(table.create_snapshot(SnapshotOption {
                                force_create: true,
                                skip_iceberg_snapshot: true,
                                index_merge_option: MaintenanceOption::Skip,
                                data_compaction_option: MaintenanceOption::Skip,
                            }));
                            table_handler_state.mooncake_snapshot_ongoing = true;
                            table_handler_state.finish_initial_copy();

                            // Drop any events that have LSN less than the start LSN during apply.
                            table_handler_state.initial_persistence_lsn = Some(start_lsn);
                            // Apply the buffered events.
                            Self::process_blocked_events(&mut table, &mut table_handler_state).await;
                        }
                        // ==============================
                        // Table internal events
                        // ==============================
                        //
                        TableEvent::PeriodicalMooncakeTableSnapshot => {
                            // Only create a periodic snapshot if there isn't already one in progress
                            if table_handler_state.mooncake_snapshot_ongoing {
                                continue;
                            }

                            // Check whether a flush and force snapshot is needed.
                            if table_handler_state.has_pending_force_snapshot_request() && !table_handler_state.iceberg_snapshot_ongoing {
                                if let Some(commit_lsn) = table_handler_state.table_consistent_view_lsn {
                                    Self::flush_table(&mut table, commit_lsn, &mut table_handler_state);
                                    table_handler_state.reset_iceberg_state_at_mooncake_snapshot();
                                    if let SpecialTableState::AlterTable { .. } = table_handler_state.special_table_state {
                                        table.force_empty_iceberg_payload();
                                    }
                                    assert!(table.create_snapshot(table_handler_state.get_mooncake_snapshot_option(/*request_force=*/true)));
                                    table_handler_state.mooncake_snapshot_ongoing = true;
                                    continue;
                                }
                            }

                            // Fallback to normal periodic snapshot.
                            table_handler_state.reset_iceberg_state_at_mooncake_snapshot();
                            table_handler_state.mooncake_snapshot_ongoing = table.create_snapshot(table_handler_state.get_mooncake_snapshot_option(/*request_force=*/false));
                        }
                        TableEvent::MooncakeTableSnapshotResult { lsn, iceberg_snapshot_payload, data_compaction_payload, file_indice_merge_payload, evicted_data_files_to_delete } => {
                            // Spawn a detached best-effort task to delete evicted object storage cache.
                            start_task_to_delete_evicted(evicted_data_files_to_delete);

                            // Mark mooncake snapshot as completed.
                            table.mark_mooncake_snapshot_completed();
                            table_handler_state.mooncake_snapshot_ongoing = false;

                            // Drop table if requested, and table at a clean state.
                            if table_handler_state.special_table_state == SpecialTableState::DropTable && table_handler_state.can_drop_table_now() {
                                drop_table(&mut table, event_sync_sender).await;
                                return;
                            }

                            // Notify read the mooncake table commit of LSN.
                            table.notify_snapshot_reader(lsn);

                            // Process iceberg snapshot and trigger iceberg snapshot if necessary.
                            if table_handler_state.can_initiate_iceberg_snapshot() {
                                if let Some(mut iceberg_snapshot_payload) = iceberg_snapshot_payload {
                                    // Update table maintainence status.
                                    if iceberg_snapshot_payload.contains_table_maintenance_payload() && table_handler_state.table_maintenance_process_status == MaintenanceProcessStatus::ReadyToPersist {
                                        table_handler_state.table_maintenance_process_status = MaintenanceProcessStatus::InPersist;
                                    }
                                    table_handler_state.iceberg_snapshot_ongoing = true;
                                    if table_handler_state.should_complete_alter_table(iceberg_snapshot_payload.flush_lsn) {
                                        if let SpecialTableState::AlterTable { ref mut alter_table_request, .. } = table_handler_state.special_table_state {
                                            let new_table_metadata = table.alter_table(alter_table_request.take().unwrap());
                                            iceberg_snapshot_payload.new_table_schema = Some(new_table_metadata);
                                        }
                                        else {
                                            unreachable!("alter table request is not set");
                                        }
                                        table_handler_state.finish_alter_table();
                                        Self::process_blocked_events(&mut table, &mut table_handler_state).await;
                                    }
                                    table.persist_iceberg_snapshot(iceberg_snapshot_payload);
                                }
                            }

                            // Only attempt new maintenance when there's no ongoing one.
                            if table_handler_state.table_maintenance_process_status == MaintenanceProcessStatus::Unrequested {
                                // ==========================
                                // Data compaction
                                // ==========================
                                //
                                // Unlike snapshot, we can actually have multiple data compaction operations ongoing concurrently,
                                // to simplify workflow we limit at most one ongoing.
                                //
                                // If there's force compact request, and there's nothing to compact, directly ack back.
                                if table_handler_state.data_compaction_request_status.is_force_request() && data_compaction_payload.is_nothing() {
                                    let _ = table_handler_state.table_maintenance_completion_tx.send(Ok(()));
                                    table_handler_state.data_compaction_request_status = MaintenanceRequestStatus::Unrequested;
                                }

                                // Get payload and try perform maintenance operations.
                                if let Some(data_compaction_payload) = data_compaction_payload.take_payload() {
                                    table_handler_state.table_maintenance_process_status = MaintenanceProcessStatus::InProcess;
                                    table.perform_data_compaction(data_compaction_payload);
                                }

                                // ==========================
                                // Index merge
                                // ==========================
                                //
                                // Unlike snapshot, we can actually have multiple file index merge operations ongoing concurrently,
                                // to simplify workflow we limit at most one ongoing.
                                //
                                // If there's force merge request, and there's nothing to merge, directly ack back.
                                if table_handler_state.index_merge_request_status.is_force_request() && file_indice_merge_payload.is_nothing() {
                                    let _ = table_handler_state.table_maintenance_completion_tx.send(Ok(()));
                                    table_handler_state.index_merge_request_status = MaintenanceRequestStatus::Unrequested;
                                }

                                if let Some(file_indice_merge_payload) = file_indice_merge_payload.take_payload() {
                                    assert_eq!(table_handler_state.table_maintenance_process_status, MaintenanceProcessStatus::Unrequested);
                                    table_handler_state.table_maintenance_process_status = MaintenanceProcessStatus::InProcess;
                                    table.perform_index_merge(file_indice_merge_payload);
                                }
                            }
                        }
                        TableEvent::IcebergSnapshotResult { iceberg_snapshot_result } => {
                            table_handler_state.iceberg_snapshot_ongoing = false;
                            match iceberg_snapshot_result {
                                Ok(snapshot_res) => {
                                    // Update table maintenance operation status.
                                    if table_handler_state.table_maintenance_process_status == MaintenanceProcessStatus::InPersist && snapshot_res.contains_maintanence_result() {
                                        table_handler_state.table_maintenance_process_status = MaintenanceProcessStatus::Unrequested;
                                        // Table maintenance could come from table internal events, which doesn't have notification receiver.
                                        let _ = table_handler_state.table_maintenance_completion_tx.send(Ok(()));
                                    }

                                    // Buffer iceberg persistence result, which later will be reflected to mooncake snapshot.
                                    let iceberg_flush_lsn = snapshot_res.flush_lsn;
                                    event_sync_sender.flush_lsn_tx.send(iceberg_flush_lsn).unwrap();
                                    table.set_iceberg_snapshot_res(snapshot_res);
                                    table_handler_state.iceberg_snapshot_result_consumed = false;

                                    // Notify all waiters with LSN satisfied.
                                    let replication_lsn = *replication_lsn_rx.borrow();
                                    table_handler_state.update_iceberg_persisted_lsn(iceberg_flush_lsn, replication_lsn);
                                }
                                Err(e) => {
                                    let err = Err(Error::IcebergMessage(format!("Failed to create iceberg snapshot: {e:?}")));
                                    if table_handler_state.has_pending_force_snapshot_request() {
                                        if let Err(send_err) = table_handler_state.force_snapshot_completion_tx.send(Some(err.clone())) {
                                            error!(error = ?send_err, "failed to notify force snapshot, because receive end has closed channel");
                                        }
                                    }

                                    // Update table maintainence operation status.
                                    if table_handler_state.table_maintenance_process_status == MaintenanceProcessStatus::InPersist {
                                        table_handler_state.table_maintenance_process_status = MaintenanceProcessStatus::Unrequested;
                                        // Table maintenance could come from table internal events, which doesn't have notification receiver.
                                        let _ = table_handler_state.table_maintenance_completion_tx.send(Err(e));
                                    }

                                    // If iceberg snapshot fails, send error back to all broadcast subscribers and unset force snapshot requests.
                                    table_handler_state.largest_force_snapshot_lsn = None;
                                }
                            }

                            // Drop table if requested, and table at a clean state.
                            if table_handler_state.special_table_state == SpecialTableState::DropTable && table_handler_state.can_drop_table_now() {
                                drop_table(&mut table, event_sync_sender).await;
                                return;
                            }
                        }
                        TableEvent::IndexMergeResult { index_merge_result } => {
                            table.set_file_indices_merge_res(index_merge_result);
                            table_handler_state.mark_index_merge_completed().await;
                            // Check whether need to drop table.
                            if table_handler_state.special_table_state == SpecialTableState::DropTable && table_handler_state.can_drop_table_now() {
                                drop_table(&mut table, event_sync_sender).await;
                                return;
                            }
                        }
                        TableEvent::DataCompactionResult { data_compaction_result } => {
                            table_handler_state.mark_data_compaction_completed(&data_compaction_result).await;
                            match data_compaction_result {
                                Ok(data_compaction_res) => {
                                    table.set_data_compaction_res(data_compaction_res)
                                }
                                Err(err) => {
                                    error!(error = ?err, "failed to perform compaction");
                                }
                            }
                            // Check whether need to drop table.
                            if table_handler_state.special_table_state == SpecialTableState::DropTable && table_handler_state.can_drop_table_now() {
                                drop_table(&mut table, event_sync_sender).await;
                                return;
                            }
                        }
                        TableEvent::FlushResult { xact_id, flush_result, lsn } => {
                            Self::set_flush_result(&mut table, lsn, &mut table_handler_state, flush_result, xact_id);
                        }
                        TableEvent::ReadRequestCompletion { cache_handles } => {
                            table.set_read_request_res(cache_handles);
                        }
                        TableEvent::EvictedDataFilesToDelete { evicted_data_files } => {
                            start_task_to_delete_evicted(evicted_data_files);
                        }
                        // ==============================
                        // Replication events
                        // ==============================
                        //
                        _ => {
                            unreachable!("unexpected event: {:?}", event);
                        }
                    }
                }
                // If all senders have been dropped, exit the loop
                else => {
                    if let Err(e) = table.shutdown().await {
                        error!(error = %e, "failed to shutdown table");
                    }
                    debug!("all event senders dropped, shutting down table handler");
                    break;
                }
            }
        }
    }

    #[allow(clippy::too_many_arguments)]
    async fn process_cdc_table_event(
        event: TableEvent,
        table: &mut MooncakeTable,
        table_handler_state: &mut TableHandlerState,
    ) {
        // ==============================
        // Replication events
        // ==============================
        //
        let is_initial_copy_event = matches!(
            event,
            TableEvent::Append {
                is_copied: true,
                ..
            }
        );

        if table_handler_state.is_in_blocking_state() && !is_initial_copy_event {
            table_handler_state.initial_copy_buffered_events.push(event);
            return;
        }

        // In the case that this is an initial copy event we acutally expect the LSN to be less than the initial persistence LSN, hence we don't discard it.
        if table_handler_state.should_discard_event(&event) && !is_initial_copy_event {
            return;
        }
        assert_eq!(
            is_initial_copy_event,
            table_handler_state.special_table_state == SpecialTableState::InitialCopy
        );

        match event {
            TableEvent::Append {
                is_copied,
                row,
                xact_id,
                ..
            } => {
                if is_copied {
                    if let Err(e) = table.append_in_stream_batch(row, INITIAL_COPY_XACT_ID) {
                        error!(error = %e, "failed to append row");
                    }
                    return;
                }

                let result = match xact_id {
                    Some(xact_id) => {
                        let res = table.append_in_stream_batch(row, xact_id);
                        if table.should_transaction_flush(xact_id) {
                            if let Err(e) = table.flush(None, Some(xact_id)) {
                                error!(error = %e, "failed to flush transaction");
                            }
                        }
                        res
                    }
                    None => table.append(row),
                };

                if let Err(e) = result {
                    error!(error = %e, "failed to append row");
                }
            }
            TableEvent::Delete { row, lsn, xact_id } => {
                match xact_id {
                    Some(xact_id) => table.delete_in_stream_batch(row, xact_id).await,
                    None => table.delete(row, lsn).await,
                };
            }
            TableEvent::Commit { lsn, xact_id } => {
                Self::commit_and_attempt_flush(
                    lsn,
                    xact_id,
                    table_handler_state,
                    table,
                    /*force_flush_requested=*/ false,
                )
                .await;
            }
            TableEvent::StreamAbort { xact_id } => {
                table.abort_in_stream_batch(xact_id);
            }
            TableEvent::CommitFlush { lsn, xact_id } => {
                Self::commit_and_attempt_flush(
                    lsn,
                    xact_id,
                    table_handler_state,
                    table,
                    /*force_flush_requested=*/ true,
                )
                .await;
            }
            TableEvent::StreamFlush { xact_id } => {
                if let Err(e) = table.flush(None, Some(xact_id)) {
                    error!(error = %e, "failed to flush transaction");
                }
            }
            _ => {
                unreachable!("unexpected event: {:?}", event)
            }
        }
    }

    /// Set flush result, which will append the flushed disk slice into current task.
    /// Also remove the LSN from pending flush LSNs.
    pub(crate) fn set_flush_result(
        table: &mut MooncakeTable,
        lsn: Option<u64>,
        table_handler_state: &mut TableHandlerState,
        flush_result: Option<Result<DiskSliceWriter>>,
        xact_id: Option<u32>,
    ) {
        if let Some(lsn) = lsn {
            table.set_flush_lsn(lsn);
        }
        match flush_result {
            Some(Ok(mut disk_slice)) => {
                // Add the flushed files from the disk slice.
                // If this is a stream commit, add them to the stream state, otherwise add them direclty to the next snapshot task.
                if let Some(xact_id) = xact_id {
                    let stream_state = table.get_transaction_stream_state(xact_id).unwrap();
                    for (file, file_attrs) in disk_slice.take_output_files() {
                        ma::assert_gt!(file_attrs.file_size, 0);
                        let disk_file_entry = DiskFileEntry {
                            file_size: file_attrs.file_size,
                            cache_handle: None,
                            batch_deletion_vector: BatchDeletionVector::new(file_attrs.row_num),
                            puffin_deletion_blob: None,
                        };
                        stream_state.insert_flushed_file(file.clone(), disk_file_entry);
                    }
                    let index = disk_slice.take_index();
                    if let Some(index) = index {
                        stream_state.insert_flushed_file_index(index);
                    }
                } else {
                    table.add_new_disk_slice(disk_slice);
                }

                // Check if this is a stream commit.
                if let (Some(lsn), Some(xact_id)) = (lsn, xact_id) {
                    // If both lsn and xact_id are provided, it means this is a stream commit.
                    // We need to do all of the post-flush commit processing here.
                    let stream_state = table.remove_transaction_stream_state(xact_id);

                    let commit = TransactionStreamCommit {
                        xact_id,
                        commit_lsn: lsn,
                        flushed_file_index: stream_state.flushed_file_index,
                        flushed_files: stream_state.flushed_files,
                        local_deletions: stream_state.local_deletions,
                        pending_deletions: stream_state.pending_deletions_in_main_mem_slice,
                    };
                    table.add_stream_commit_to_next_snapshot_task(commit);
                }
            }
            Some(Err(e)) => {
                error!(error = %e, "failed to flush mem slice");
            }
            None => {
                debug!("flush result is none");
            }
        }
        if let Some(lsn) = lsn {
            table_handler_state.pending_flush_lsns.remove(&lsn);
        }
    }

    /// Adds flush LSN to pending flush LSNs, and flush the table.
    /// We should only flush the table using this function, and not directly call `table.flush(lsn)`.
    fn flush_table(
        table: &mut MooncakeTable,
        lsn: u64,
        table_handler_state: &mut TableHandlerState,
    ) {
        table_handler_state.pending_flush_lsns.insert(lsn);
        if let Err(e) = table.flush(Some(lsn), None) {
            error!(error = %e, "flush failed in commit");
        }
    }

    async fn process_blocked_events(
        table: &mut MooncakeTable,
        table_handler_state: &mut TableHandlerState,
    ) {
        let buffered_events = table_handler_state
            .initial_copy_buffered_events
            .drain(..)
            .collect::<Vec<_>>();
        for event in buffered_events {
            Self::process_cdc_table_event(event, table, table_handler_state).await;
        }
    }

    async fn commit_and_attempt_flush(
        lsn: u64,
        xact_id: Option<u32>,
        table_handler_state: &mut TableHandlerState,
        table: &mut MooncakeTable,
        force_flush_requested: bool,
    ) {
        // Force create snapshot if
        // 1. force snapshot is requested
        // and 2. LSN which meets force snapshot requirement has appeared, before that we still allow buffering
        // and 3. there's no snapshot creation operation ongoing

        let should_force_snapshot = table_handler_state.should_force_snapshot_by_commit_lsn(lsn);

        match xact_id {
            Some(xact_id) => {
                // For streaming writers, whose commit LSN is only finalized at commit phase, delay decision whether to discard now.
                // If commit LSN is no fresher than persistence LSN, it means already persisted, directly discard.
                if let Some(initial_persistence_lsn) = table_handler_state.initial_persistence_lsn {
                    if lsn <= initial_persistence_lsn {
                        table.abort_in_stream_batch(xact_id);
                        return;
                    }
                }
                if let Err(e) = table.commit_transaction_stream(xact_id, lsn) {
                    error!(error = %e, "stream commit flush failed");
                }
            }
            None => {
                table.commit(lsn);
                if table.should_flush() || should_force_snapshot || force_flush_requested {
                    Self::flush_table(table, lsn, table_handler_state);
                }
            }
        }

        if should_force_snapshot {
            table_handler_state.reset_iceberg_state_at_mooncake_snapshot();
            assert!(table.create_snapshot(
                table_handler_state.get_mooncake_snapshot_option(/*request_force=*/ true)
            ));
            table_handler_state.mooncake_snapshot_ongoing = true;
        }
    }
}
#[cfg(test)]
mod tests;

#[cfg(test)]
mod test_utils;

#[cfg(test)]
mod failure_tests;

#[cfg(test)]
#[cfg(feature = "chaos-test")]
mod chaos_test;
